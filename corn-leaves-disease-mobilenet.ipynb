{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:12:07.434333Z","iopub.execute_input":"2022-05-28T00:12:07.435141Z","iopub.status.idle":"2022-05-28T00:12:08.319485Z","shell.execute_reply.started":"2022-05-28T00:12:07.435028Z","shell.execute_reply":"2022-05-28T00:12:08.318496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Identify Plant Diseases\n\nWe use the PlantVillage dataset [1] by Hughes et al. consists of about 87,000 healthy and unhealthy leaf images divided into 38 categories by species and disease. Here we provide a subset of our experiments on working with this data. We also end up transfer learning from MobileNet and use the weights from pre-training on ImageNet.\n\n* ![PlantVillage Dataset Samples](https://i.imgur.com/Zcxdrlc.png)\nFigure 1. PlantVillage Dataset Samples\n\n## Classes\n\nThe following 38 classes are availaible in the dataset\n\n- `Apple___Apple_scab` \n- `Apple___Black_rot` \n- `Apple___Cedar_apple_rust` \n- `Apple___healthy` \n- `Blueberry___healthy` \n- `Cherry_(including_sour)___Powdery_mildew` \n- `Cherry_(including_sour)___healthy` \n- `Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot` \n- `Corn_(maize)___Common_rust_` \n- `Corn_(maize)___Northern_Leaf_Blight` \n- `Corn_(maize)___healthy', 'Grape___Black_rot` \n- `Grape___Leaf_blight_(Isariopsis_Leaf_Spot)` \n- `Grape___healthy` \n- `Orange___Haunglongbing_(Citrus_greening)` \n- `Peach___Bacterial_spot` \n- `Peach___healthy` \n- `Pepper,_bell___Bacterial_spot` \n- `Pepper,_bell___healthy` \n- `Potato___Early_blight` \n- `Potato___Late_blight` \n- `Potato___healthy` \n- `Raspberry___healthy` \n- `Soybean___healthy` \n- `Squash___Powdery_mildew` \n- `Strawberry___Leaf_scorch` \n- `Strawberry___healthy` \n- `Tomato___Bacterial_spot` \n- `Tomato___Late_blight` \n- `Tomato___Leaf_Mold` \n- `Tomato___Septoria_leaf_spot` \n- `Tomato___Spider_mites Two-spotted_spider_mite` \n- `Tomato___Target_Spot` \n- `Tomato___Tomato_Yellow_Leaf_Curl_Virus` \n- `Tomato___Tomato_mosaic_virus` \n- `Tomato___healthy`\n\n## References\n\n[1] Hughes, David P., and Marcel Salathe. “An Open Access Repository of Images on Plant Health to Enable the Development of Mobile Disease Diagnostics.” ArXiv:1511.08060 [Cs], Apr. 2016. arXiv.org, http://arxiv.org/abs/1511.08060.\n\n[2] Howard, Andrew G., et al. “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.” ArXiv:1704.04861 [Cs], Apr. 2017. arXiv.org, http://arxiv.org/abs/1704.04861.\n","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:12:22.024361Z","iopub.execute_input":"2022-05-28T00:12:22.024754Z","iopub.status.idle":"2022-05-28T00:12:27.543741Z","shell.execute_reply.started":"2022-05-28T00:12:22.024722Z","shell.execute_reply":"2022-05-28T00:12:27.542936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_size = 224\ntarget_size = (image_size, image_size)\ninput_shape = (image_size, image_size, 3)\n\nbatch_size = 32\nepochs = 25","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:13:23.528555Z","iopub.execute_input":"2022-05-28T00:13:23.528902Z","iopub.status.idle":"2022-05-28T00:13:23.533282Z","shell.execute_reply.started":"2022-05-28T00:13:23.528871Z","shell.execute_reply":"2022-05-28T00:13:23.532433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get data\n\nIn order to make the most of our few training examples, we will \"augment\" them via a number of random transformations, so that our model would never see twice the exact same picture. This helps prevent overfitting and helps the model generalize better.\n\nIn TensorFlow this can be done via the `tf.keras.preprocessing.image.ImageDataGenerator` class. This class allows you to:\n\n- configure random transformations and normalization operations to be done on your image data during training\n- instantiate generators of augmented image batches (and their labels) via `.flow(data, labels)` or `.flow_from_directory(directory)`. These generators can then be used with the `tf.keras` model methods that accept data generators as inputs, `fit`, `evaluate` and `predict`.","metadata":{}},{"cell_type":"code","source":"base_dir = \"../input/new-plant-diseases-dataset/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)\"\ntrain_dir = os.path.join(base_dir,\"train\")\ntest_dir = os.path.join(base_dir,\"valid\")","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:13:27.199241Z","iopub.execute_input":"2022-05-28T00:13:27.199579Z","iopub.status.idle":"2022-05-28T00:13:27.204615Z","shell.execute_reply.started":"2022-05-28T00:13:27.199548Z","shell.execute_reply":"2022-05-28T00:13:27.203746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We make the following augmentations to the images:\n\n- `width_shift` and `height_shift` are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally\n- `rescale` is a value by which we will multiply the data before any other processing. Our original images consist in RGB coefficients in the 0-255, but such values would be too high for our models to process (given a typical learning rate), so we target values between 0 and 1 instead by scaling with a 1/255. factor.\n- `shear_range` is for randomly applying shearing transformations\n- `zoom_range` is for randomly zooming inside pictures\n- `fill_mode` is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.","metadata":{}},{"cell_type":"code","source":"train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.0,\n                                                             shear_range = 0.2,\n                                                             zoom_range = 0.2,\n                                                             width_shift_range = 0.2,\n                                                             height_shift_range = 0.2,\n                                                             fill_mode=\"nearest\")\n\ntest_datagen = keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.0)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:13:29.48195Z","iopub.execute_input":"2022-05-28T00:13:29.48232Z","iopub.status.idle":"2022-05-28T00:13:29.487662Z","shell.execute_reply.started":"2022-05-28T00:13:29.482288Z","shell.execute_reply":"2022-05-28T00:13:29.486572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's prepare our data. We will use `.flow_from_directory()` to generate batches of image data (and their labels) directly from our images in their respective folders.","metadata":{}},{"cell_type":"code","source":"train_data = train_datagen.flow_from_directory(train_dir,\n                                               target_size = (image_size, image_size),\n                                               batch_size = batch_size,\n                                               class_mode = \"categorical\")\n\ntest_data = test_datagen.flow_from_directory(test_dir,\n                                             target_size = (image_size, image_size),\n                                             batch_size = batch_size,\n                                             class_mode = \"categorical\")","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:13:31.634094Z","iopub.execute_input":"2022-05-28T00:13:31.634574Z","iopub.status.idle":"2022-05-28T00:14:44.644746Z","shell.execute_reply.started":"2022-05-28T00:13:31.634524Z","shell.execute_reply":"2022-05-28T00:14:44.643746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a classes index file\n\nWe also want to know which class corresponds to which species and disease so we also create a `json` file which shows corresponding labels and class indices.","metadata":{}},{"cell_type":"code","source":"categories = list(train_data.class_indices.keys())\nprint(train_data.class_indices)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:14:44.646588Z","iopub.execute_input":"2022-05-28T00:14:44.647112Z","iopub.status.idle":"2022-05-28T00:14:44.653996Z","shell.execute_reply.started":"2022-05-28T00:14:44.647073Z","shell.execute_reply":"2022-05-28T00:14:44.653036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nwith open('class_indices.json','w') as f:\n  json.dump(train_data.class_indices, f)\n\nfrom IPython.display import FileLink\nFileLink(r'class_indices.json')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:14:44.655402Z","iopub.execute_input":"2022-05-28T00:14:44.655815Z","iopub.status.idle":"2022-05-28T00:14:44.669209Z","shell.execute_reply.started":"2022-05-28T00:14:44.655778Z","shell.execute_reply":"2022-05-28T00:14:44.668206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nWe first get the base MobileNet model without including the top layers since we want to use it for 38 classes and us the pre-trained weights for ImageNet.","metadata":{}},{"cell_type":"code","source":"base_model = tf.keras.applications.MobileNet(weights = \"imagenet\",\n                                             include_top = False,\n                                             input_shape = input_shape)\n\nbase_model.trainable = False","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:14:44.670781Z","iopub.execute_input":"2022-05-28T00:14:44.671643Z","iopub.status.idle":"2022-05-28T00:14:48.573473Z","shell.execute_reply.started":"2022-05-28T00:14:44.671549Z","shell.execute_reply":"2022-05-28T00:14:48.572556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now create a small upstream model on top of the MobileNet using the functional API","metadata":{}},{"cell_type":"code","source":"inputs = keras.Input(shape = input_shape)\n\nx = base_model(inputs, training = False)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(len(categories), \n                          activation=\"softmax\")(x)\n\nmodel = keras.Model(inputs = inputs, \n                    outputs = x, \n                    name=\"LeafDisease_MobileNet\")","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:14:48.575998Z","iopub.execute_input":"2022-05-28T00:14:48.576608Z","iopub.status.idle":"2022-05-28T00:14:48.84391Z","shell.execute_reply.started":"2022-05-28T00:14:48.576569Z","shell.execute_reply":"2022-05-28T00:14:48.842913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In our multiple experiments we found out Adam optimizer to work really well with it's default learning rate, $\\beta_1$, $\\beta_2$ and $\\epsilon$ values","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\n\nmodel.compile(optimizer = optimizer,\n              loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True),\n              metrics=[keras.metrics.CategoricalAccuracy(), \n                       'accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:14:48.845569Z","iopub.execute_input":"2022-05-28T00:14:48.846152Z","iopub.status.idle":"2022-05-28T00:14:48.879715Z","shell.execute_reply.started":"2022-05-28T00:14:48.846115Z","shell.execute_reply":"2022-05-28T00:14:48.878871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_data,\n                    validation_data=test_data,\n                    epochs=epochs,\n                    steps_per_epoch=100,\n                    validation_steps=100)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:14:48.881056Z","iopub.execute_input":"2022-05-28T00:14:48.881627Z","iopub.status.idle":"2022-05-28T00:41:27.452513Z","shell.execute_reply.started":"2022-05-28T00:14:48.881592Z","shell.execute_reply":"2022-05-28T00:41:27.451755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Review the training process","metadata":{}},{"cell_type":"code","source":"epochs","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:55:10.986445Z","iopub.execute_input":"2022-05-28T00:55:10.986849Z","iopub.status.idle":"2022-05-28T00:55:10.99356Z","shell.execute_reply.started":"2022-05-28T00:55:10.98682Z","shell.execute_reply":"2022-05-28T00:55:10.992575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(loss))\n\nfig = plt.figure(figsize=(10,6))\nplt.plot(epochs,loss,c=\"red\",label=\"Training\")\nplt.plot(epochs,val_loss,c=\"blue\",label=\"Validation\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:41:27.455719Z","iopub.execute_input":"2022-05-28T00:41:27.455978Z","iopub.status.idle":"2022-05-28T00:41:27.632589Z","shell.execute_reply.started":"2022-05-28T00:41:27.455953Z","shell.execute_reply":"2022-05-28T00:41:27.631777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\n\nepochs = range(len(acc))\n\nfig = plt.figure(figsize=(10,6))\nplt.plot(epochs,acc,c=\"red\",label=\"Training\")\nplt.plot(epochs,val_acc,c=\"blue\",label=\"Validation\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:41:27.63399Z","iopub.execute_input":"2022-05-28T00:41:27.634332Z","iopub.status.idle":"2022-05-28T00:41:27.803582Z","shell.execute_reply.started":"2022-05-28T00:41:27.634294Z","shell.execute_reply":"2022-05-28T00:41:27.802712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the model\n\nWe finally save the model in the standard TensorFlow 2 SavedModel format.","metadata":{}},{"cell_type":"code","source":"model.save('plant_disease')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T12:43:24.397718Z","iopub.status.idle":"2022-05-22T12:43:24.398485Z"},"trusted":true},"execution_count":null,"outputs":[]}]}