{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"corn-leaves-disease-mobilenet.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Clasificación de enfermedades en hojas de maíz\n","\n","Hemos utilizado la base de datos PlantVillage [1] elaborada por Hughes et al. la cual consiste en alrededor de 87,000 saludables y no saludables imagenes de hojas de plantas dividida en 38 categorias por especie y enfermedad. Se utiliza la técnica de \"transfer learning\" para la red MobileNet, además se utilizan los pesos de un pre-entrenamiento utilizando ImageNet.\n","\n","* ![PlantVillage Dataset Samples](https://i.imgur.com/Zcxdrlc.png)\n","Figure 1. Ejemplos de la base de datos PlantVillage \n","\n","## Referencias\n","\n","[1] Hughes, David P., and Marcel Salathe. “An Open Access Repository of Images on Plant Health to Enable the Development of Mobile Disease Diagnostics.” ArXiv:1511.08060 [Cs], Apr. 2016. arXiv.org, http://arxiv.org/abs/1511.08060.\n"],"metadata":{"id":"IFGWM2ICv7M3"}},{"cell_type":"markdown","source":["## Configuración Inicial"],"metadata":{"id":"K2FHaSBbv7M6"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import os"],"metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:12:22.024361Z","iopub.execute_input":"2022-05-28T00:12:22.024754Z","iopub.status.idle":"2022-05-28T00:12:27.543741Z","shell.execute_reply.started":"2022-05-28T00:12:22.024722Z","shell.execute_reply":"2022-05-28T00:12:27.542936Z"},"trusted":true,"id":"R7Na-2AIv7M7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_size = 224\n","target_size = (image_size, image_size)\n","input_shape = (image_size, image_size, 3)\n","\n","batch_size = 32\n","epochs = 25"],"metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:13:23.528555Z","iopub.execute_input":"2022-05-28T00:13:23.528902Z","iopub.status.idle":"2022-05-28T00:13:23.533282Z","shell.execute_reply.started":"2022-05-28T00:13:23.528871Z","shell.execute_reply":"2022-05-28T00:13:23.532433Z"},"trusted":true,"id":"-miTN78Tv7M8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Obtención de datos\n","\n","A fin de aprovechar al máximo las muestras de entrenamiento de la base de datos, procedemos a realizar un aumento de los mismos a través de una serie de transformaciones aleatorias, de modo que el modelo MobileNet propuesto nunca ve la misma imagen dos veces. Esto ayuda a evitar el sobreajuste y ayuda a generalizar mejor el modelo.\n","\n","En TensorFlow, esto se puede hacer a través de la clase `tf.keras.preprocessing.image.ImageDataGenerator`. Esta clase permite:\n","\n","- configurar las transformaciones aleatorias y operaciones de normalización que se realizarán en las imagenes durante el entrenamiento.\n","- crear instancias de generadores de lotes (batchs) de imágenes aumentadas (y sus etiquetas) a través de `.flow(datos, etiquetas)` o `.flow_from_directory(directory)`. Estos generadores se pueden usar con los métodos del modelo `tf.keras` que aceptan generadores de datos como entradas, `ajuste (fit)`, `evaluar (evaluate)` y `predecir (predecir)`."],"metadata":{"id":"cycavtBBv7M9"}},{"cell_type":"code","source":["base_dir = \"../input/new-plant-diseases-dataset/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)\"\n","train_dir = os.path.join(base_dir,\"train\")\n","test_dir = os.path.join(base_dir,\"valid\")"],"metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:13:27.199241Z","iopub.execute_input":"2022-05-28T00:13:27.199579Z","iopub.status.idle":"2022-05-28T00:13:27.204615Z","shell.execute_reply.started":"2022-05-28T00:13:27.199548Z","shell.execute_reply":"2022-05-28T00:13:27.203746Z"},"trusted":true,"id":"99TdnM9Yv7M9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hemos realizado los siguientes aumentos a las imágenes:\n","\n","- `width_shift` y `height_shift` son rangos (como una fracción del ancho o alto total) dentro de los cuales se pueden trasladar aleatoriamente imágenes vertical u horizontalmente.\n","- `rescale` es un valor por el que multiplicaremos los datos antes de cualquier otro procesamiento. Las imágenes originales consisten en coeficientes RGB en el rango de 0-255, pero tales valores serían demasiado altos para que los modelos los procesen, por lo que apuntamos a valores entre 0 y 1 escalando con un factor 1/255. \n","- `shear_range` permite aplicar aleatoriamente transformaciones de corte\n","- `zoom_range` permite hacer zoom al azar dentro de las imágenes\n","- `fill_mode` es la estrategia utilizada para rellenar píxeles recién creados, que pueden aparecer después de una rotación o un cambio de ancho/alto.\n"],"metadata":{"id":"_K8Y5XXuv7M-"}},{"cell_type":"code","source":["train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.0,\n","                                                             shear_range = 0.2,\n","                                                             zoom_range = 0.2,\n","                                                             width_shift_range = 0.2,\n","                                                             height_shift_range = 0.2,\n","                                                             fill_mode=\"nearest\")\n","\n","test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.0)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:13:29.48195Z","iopub.execute_input":"2022-05-28T00:13:29.48232Z","iopub.status.idle":"2022-05-28T00:13:29.487662Z","shell.execute_reply.started":"2022-05-28T00:13:29.482288Z","shell.execute_reply":"2022-05-28T00:13:29.486572Z"},"trusted":true,"id":"cg_7Scz3v7M-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Posteriormente, preparamos los datos para lo cual usamos `.flow_from_directory()` para generar lotes (batches) de datos de imagen (y sus etiquetas) directamente desde las imágenes.\n"],"metadata":{"id":"WhHrH25Fv7M_"}},{"cell_type":"code","source":["train_data = train_datagen.flow_from_directory(train_dir,\n","                                               target_size = (image_size, image_size),\n","                                               batch_size = batch_size,\n","                                               class_mode = \"categorical\")\n","\n","test_data = test_datagen.flow_from_directory(test_dir,\n","                                             target_size = (image_size, image_size),\n","                                             batch_size = batch_size,\n","                                             class_mode = \"categorical\")"],"metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:13:31.634094Z","iopub.execute_input":"2022-05-28T00:13:31.634574Z","iopub.status.idle":"2022-05-28T00:14:44.644746Z","shell.execute_reply.started":"2022-05-28T00:13:31.634524Z","shell.execute_reply":"2022-05-28T00:14:44.643746Z"},"trusted":true,"id":"YwRed_SDv7M_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Crear un archivo con las clases\n","\n","Tiene por objetivo saber a qué clase corresponde y a qué especie y enfermedad, por lo que creamos un archivo `json` que muestra las etiquetas correspondientes y los índices de clase.\n"],"metadata":{"id":"Blp2HJfkv7NA"}},{"cell_type":"code","source":["categories = list(train_data.class_indices.keys())\n","print(train_data.class_indices)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:14:44.646588Z","iopub.execute_input":"2022-05-28T00:14:44.647112Z","iopub.status.idle":"2022-05-28T00:14:44.653996Z","shell.execute_reply.started":"2022-05-28T00:14:44.647073Z","shell.execute_reply":"2022-05-28T00:14:44.653036Z"},"trusted":true,"id":"vpVJDA7Ev7NA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","with open('class_indices.json','w') as f:\n","  json.dump(train_data.class_indices, f)\n","\n","from IPython.display import FileLink\n","FileLink(r'class_indices.json')"],"metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:14:44.655402Z","iopub.execute_input":"2022-05-28T00:14:44.655815Z","iopub.status.idle":"2022-05-28T00:14:44.669209Z","shell.execute_reply.started":"2022-05-28T00:14:44.655778Z","shell.execute_reply":"2022-05-28T00:14:44.668206Z"},"trusted":true,"id":"Ui4EpDmbv7NB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Entrenamiento (Training)\n","\n","Primeramente, obtenemos el modelo base de MobileNet sin incluir las capas superiores, ya que queremos usarlo para 4 clases, que representan 3 enfermedades (Cercospora Gray Leaf Spot, Common Rust y  Northern Leaf Blight) y la clase de hojas saludable (Healthy-corn) para las hojas de maíz. Posteriormente, usamos los pesos pre-entrenados de ImageNet.\n","\n"],"metadata":{"id":"-78YbDT9v7NB"}},{"cell_type":"code","source":["base_model = tf.keras.applications.MobileNet(weights = \"imagenet\",\n","                                             include_top = False,\n","                                             input_shape = input_shape)\n","\n","base_model.trainable = False"],"metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:14:44.670781Z","iopub.execute_input":"2022-05-28T00:14:44.671643Z","iopub.status.idle":"2022-05-28T00:14:48.573473Z","shell.execute_reply.started":"2022-05-28T00:14:44.671549Z","shell.execute_reply":"2022-05-28T00:14:48.572556Z"},"trusted":true,"id":"yUQ6za0Zv7NB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En forma seguida, creamos un pequeño modelo ascendente sobre MobileNet"],"metadata":{"id":"r68UdaFvv7NC"}},{"cell_type":"code","source":["inputs = keras.Input(shape = input_shape)\n","\n","x = base_model(inputs, training = False)\n","x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","x = tf.keras.layers.Dense(len(categories), \n","                          activation=\"softmax\")(x)\n","\n","model = keras.Model(inputs = inputs, \n","                    outputs = x, \n","                    name=\"LeafDisease_MobileNet\")"],"metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:14:48.575998Z","iopub.execute_input":"2022-05-28T00:14:48.576608Z","iopub.status.idle":"2022-05-28T00:14:48.84391Z","shell.execute_reply.started":"2022-05-28T00:14:48.576569Z","shell.execute_reply":"2022-05-28T00:14:48.842913Z"},"trusted":true,"id":"wc7Gnrzuv7NC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En los experimento realizados, determinamos que el optimizador de Adam funciona muy bien con tasa de aprendizaje = 0.001, los valores $\\beta_1 = 0.9$, $\\beta_2=0.999$ y $\\epsilon=1^{-8}$ "],"metadata":{"id":"2KYf64YTv7NC"}},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam()\n","\n","model.compile(optimizer = optimizer,\n","              loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True),\n","              metrics=[keras.metrics.CategoricalAccuracy(), \n","                       'accuracy'])"],"metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:14:48.845569Z","iopub.execute_input":"2022-05-28T00:14:48.846152Z","iopub.status.idle":"2022-05-28T00:14:48.879715Z","shell.execute_reply.started":"2022-05-28T00:14:48.846115Z","shell.execute_reply":"2022-05-28T00:14:48.878871Z"},"trusted":true,"id":"NdrSSrmzv7NC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(train_data,\n","                    validation_data=test_data,\n","                    epochs=epochs,\n","                    steps_per_epoch=100,\n","                    validation_steps=100)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:14:48.881056Z","iopub.execute_input":"2022-05-28T00:14:48.881627Z","iopub.status.idle":"2022-05-28T00:41:27.452513Z","shell.execute_reply.started":"2022-05-28T00:14:48.881592Z","shell.execute_reply":"2022-05-28T00:41:27.451755Z"},"trusted":true,"id":"vIUIL6kiv7ND"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Validando el proceso de entrenamiento"],"metadata":{"id":"CcM0-NP5v7ND"}},{"cell_type":"code","source":["loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(loss))\n","\n","fig = plt.figure(figsize=(10,6))\n","plt.plot(epochs,loss,c=\"red\",label=\"Training\")\n","plt.plot(epochs,val_loss,c=\"blue\",label=\"Validation\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()"],"metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:41:27.455719Z","iopub.execute_input":"2022-05-28T00:41:27.455978Z","iopub.status.idle":"2022-05-28T00:41:27.632589Z","shell.execute_reply.started":"2022-05-28T00:41:27.455953Z","shell.execute_reply":"2022-05-28T00:41:27.631777Z"},"trusted":true,"id":"XW2du3lkv7ND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc = history.history['categorical_accuracy']\n","val_acc = history.history['val_categorical_accuracy']\n","\n","epochs = range(len(acc))\n","\n","fig = plt.figure(figsize=(10,6))\n","plt.plot(epochs,acc,c=\"red\",label=\"Training\")\n","plt.plot(epochs,val_acc,c=\"blue\",label=\"Validation\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()"],"metadata":{"execution":{"iopub.status.busy":"2022-05-28T00:41:27.63399Z","iopub.execute_input":"2022-05-28T00:41:27.634332Z","iopub.status.idle":"2022-05-28T00:41:27.803582Z","shell.execute_reply.started":"2022-05-28T00:41:27.634294Z","shell.execute_reply":"2022-05-28T00:41:27.802712Z"},"trusted":true,"id":"_abnruo8v7NE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Guardando el model entrenado\n","\n","Finalmente, guardamos el modelo en el formato estándar TensorFlow 2 SavedModel."],"metadata":{"id":"nb07Izcov7NE"}},{"cell_type":"code","source":["model.save('plant_disease')"],"metadata":{"execution":{"iopub.status.busy":"2022-05-22T12:43:24.397718Z","iopub.status.idle":"2022-05-22T12:43:24.398485Z"},"trusted":true,"id":"nA6PUkcSv7NE"},"execution_count":null,"outputs":[]}]}